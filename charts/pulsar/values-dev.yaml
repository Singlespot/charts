#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Flag to control whether to run initialize job
initialize: true

###
### K8S Settings
###

## Namespace to deploy pulsar
namespace: pulsar-dev
namespaceCreate: false

###
### Global Settings
###

## Pulsar Metadata Prefix
##
## By default, pulsar stores all the metadata at root path.
## You can configure to have a prefix (e.g. "/my-pulsar-cluster").
## If you do so, all the pulsar and bookkeeper metadata will
## be stored under the provided path
metadataPrefix: ""

## Persistence
##
## If persistence is enabled, components that have state will
## be deployed with PersistentVolumeClaims, otherwise, for test
## purposes, they will be deployed with emptyDir
##
## This is a global setting that is applied to all components.
## If you need to disable persistence for a component,
## you can set the `volume.persistence` setting to `false` for
## that component.
volumes:
  persistence: true
  # configure the components to use local persistent volume
  # the local provisioner should be installed prior to enable local persistent volume
  local_storage: false

## AntiAffinity
##
## Flag to enable and disable `AntiAffinity` for all components.
## This is a global setting that is applied to all components.
## If you need to disable AntiAffinity for a component, you can set
## the `affinity.anti_affinity` settings to `false` for that component.
affinity:
  anti_affinity: true

## Components
##
## Control what components of Apache Pulsar to deploy for the cluster
components:
  # zookeeper
  zookeeper: true
  # bookkeeper
  bookkeeper: true
  # bookkeeper - autorecovery
  autorecovery: true
  # broker
  broker: true
  # functions
  functions: false
  # proxy
  proxy: true
  # toolset
  toolset: true
  # pulsar manager
  pulsar_manager: true
  # pulsar sql
  sql_worker: false
  # kop
  kop: false
  # pulsar detector
  pulsar_detector: false

## Monitoring Components
##
## Control what components of the monitoring stack to deploy for the cluster
monitoring:
  # monitoring - prometheus
  prometheus: true
  # monitoring - grafana
  grafana: true
  # monitoring - node_exporter
  node_exporter: true
  # alerting - alert-manager
  alert_manager: true
  # monitoring - loki
  loki: true
  # monitoring - datadog
  datadog: false

## Images
##
## Control what images to use for each component
images:
  zookeeper:
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
#    repository: streamnative/platform
#    tag: v1.0.0
    pullPolicy: IfNotPresent
  bookie:
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
#    repository: streamnative/platform
#    tag: v1.0.0
    pullPolicy: IfNotPresent
  presto:
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
#    repository: streamnative/platform
#    tag: v1.0.0
    pullPolicy: IfNotPresent
  autorecovery:
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
#    repository: streamnative/platform
#    tag: v1.0.0
    pullPolicy: IfNotPresent
  broker:
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
#    repository: streamnative/platform-all
#    tag: v1.0.1
    pullPolicy: IfNotPresent
  proxy:
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
#    repository: streamnative/platform
#    tag: v1.0.0
    pullPolicy: IfNotPresent
  pulsar_detector:
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
#    repository: streamnative/sn-pulsar
#    tag: 2.5.2-1
    pullPolicy: IfNotPresent
  functions:
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
#    repository: streamnative/platform
#    tag: v1.0.0
    pullPolicy: IfNotPresent
  prometheus:
    repository: prom/prometheus
    tag: v2.18.1
    pullPolicy: IfNotPresent
  alert_manager:
    repository: prom/alertmanager
    tag: v0.20.0
    pullPolicy: IfNotPresent
  grafana:
    repository: 268324876595.dkr.ecr.eu-west-1.amazonaws.com/apache-pulsar-grafana-dashboard-k8s
    tag: 0.0.8
    pullPolicy: IfNotPresent
  pulsar_manager:
    repository: streamnative/pulsar-manager
    tag: 0.3.0
    pullPolicy: IfNotPresent
    hasCommand: false
  node_exporter:
    repository: prom/node-exporter
    tag: v1.0.0
    pullPolicy: "IfNotPresent"
  nginx_ingress_controller:
    repository: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
    tag: 0.32.0
    pullPolicy: "IfNotPresent"

## TLS
## templates/tls-certs.yaml
##
## The chart is using cert-manager for provisioning TLS certs for
## brokers and proxies.
tls:
  enabled: false
  # common settings for generating certs
  common:
    # 90d
    duration: 2160h
    # 15d
    renewBefore: 360h
    organization:
      - pulsar
    keySize: 4096
    keyAlgorithm: rsa
    keyEncoding: pkcs8
  # settings for generating certs for proxy
  proxy:
    enabled: false
    cert_name: tls-proxy
  # settings for generating certs for proxy
  pulsar_detector:
    enabled: false
    cert_name: tls-pulsar-detector
  # settings for generating certs for broker
  broker:
    enabled: false
    cert_name: tls-broker
  # settings for generating certs for bookies
  bookie:
    enabled: false
    cert_name: tls-bookie
  # settings for generating certs for zookeeper
  zookeeper:
    enabled: false
    cert_name: tls-zookeeper
  # settings for generating certs for recovery
  autorecovery:
    cert_name: tls-recovery
  # settings for generating certs for toolset
  toolset:
    cert_name: tls-toolset
  pulsar_manager:
    enabled: false
    cert_name: tls-pulsar-manager

# Enable or disable broker authentication and authorization.
auth:
  authentication:
    enabled: false
    provider: "jwt"
    jwt:
      # Enable JWT authentication
      # If the token is generated by a secret key, set the usingSecretKey as true.
      # If the token is generated by a private key, set the usingSecretKey as false.
      usingSecretKey: false
  authorization:
    enabled: false
  superUsers:
    # broker to broker communication
    broker: "broker-admin"
    # proxy to broker communication
    proxy: "proxy-admin"
    # pulsar-admin client to broker/proxy communication
    client: "admin"
  # Enable vault based authentication
  vault:
    enabled: false
######################################################################
# External dependencies
######################################################################

## cert-manager
## templates/tls-cert-issuer.yaml
##
## Cert manager is used for automatically provisioning TLS certificates
## for components within a Pulsar cluster
certs:
  internal_issuer:
    enabled: false
    component: internal-cert-issuer
    type: selfsigning
  public_issuer:
    enabled: false
    component: public-cert-issuer
    type: acme
  issuers:
    selfsigning:
    acme:
      # You must replace this email address with your own.
      # Let's Encrypt will use this to contact you about expiring
      # certificates, and issues related to your account.
      email: contact@example.local
      # change this to production endpoint once you successfully test it
      # server: https://acme-v02.api.letsencrypt.org/directory
      server: https://acme-staging-v02.api.letsencrypt.org/directory
      solver: clouddns
      solvers:
        clouddns:
          # TODO: add a link about how to configure this section
          project: "[YOUR GCP PROJECT ID]"
          accessKeyID: [accessKeyID of the user who is able to manage the external DNS]
          serviceAccountSecretRef:
            name: "[NAME OF SECRET]"
            key: "[KEY OF SECRET]"
        # route53:
        #   region: "[ROUTE53 REGION]"
        #   secretAccessKeySecretRef:
        #     name: "[NAME OF SECRET]"
        #     key: "[KEY OF SECRET]"
        #   role: "[ASSUME A ROLE]"
  lets_encrypt:
    ca_ref:
      secretName: "[SECRET STORES lets encrypt CA]"
      keyName: "[KEY IN THE SECRET STORES let encrypt CA]"

## External DNS
## templates/external-dns.yaml
## templates/external-dns-rbac.yaml
##
## External DNS is used for synchronizing exposed Ingresses with DNS providers
external_dns:
  enabled: false
  component: external-dns
  policy: upsert-only
  registry: txt
  owner_id: pulsar
  domain_filter: pulsar.example.local
  provider: google
  providers:
    google:
      # project: external-dns-test
      project: "[GOOGLE PROJECT ID]"
    aws:
      zoneType: public
  serviceAcct:
    annotations: {}
  securityContext: {}

## Ingresses for exposing Pulsar services
ingress:
  ## templates/proxy-service-ingress.yaml
  ##
  ## Ingresses for exposing pulsar service publicly
  proxy:
    enabled: false
    tls:
      enabled: true
    type: LoadBalancer
    annotations: {}
    extraSpec: {}
  ## templates/control-center-ingress.yaml
  ##
  ## Ingresses for exposing monitoring/management services publicly
  controller:
    enabled: false
    rbac: true
    component: nginx-ingress-controller
    replicaCount: 1
    # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
    tolerations: []
    gracePeriod: 300
    annotations: {}
    ports:
      http: 80
      https: 443
    # flag whether to terminate the tls at the loadbalancer level
    tls:
      termination: false
  control_center:
    enabled: false
    component: control-center
    endpoints:
      grafana: true
      prometheus: false
      alertmanager: false
    # Set external domain of the load balancer of ingress controller
    # external_domain: your.external.control.center.domain
    # external_domain_scheme: https://
    tls:
      enabled: false
    annotations: {}

imagePuller:
  component: image-puller
  pullSecret:
    enabled: false
  hook:
    enabled: false
    image:
      name: streamnative/k8s-image-awaiter
      tag: '0.1.0'
  rbac:
    enabled: true
  continuous:
    enabled: false
  pause:
    image:
      name: gcr.io/google_containers/pause
      tag: '3.1'


######################################################################
# Below are settings for each component
######################################################################

## Common properties applied to pulsar components
common:
  extraInitContainers: {}

## Pulsar: Zookeeper cluster
## templates/zookeeper-statefulset.yaml
##
zookeeper:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: zookeeper
  # the number of zookeeper servers to run. it should be an odd number larger than or equal to 3.
  replicaCount: 3
  ports:
    metrics: 8000
    client: 2181
    clientTls: 2281
    follower: 2888
    leaderElection: 3888
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 10
      periodSeconds: 30
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 10
      periodSeconds: 30
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 10
      periodSeconds: 30
  affinity:
    anti_affinity: true
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
  securityContext: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    # Add a flag here for backward compatibility. Ideally we should
    # use two disks for production workloads. This flag might be
    # removed in the future releases to stick to two-disks mode.
    useSeparateDiskForTxlog: false
    data:
      name: data
      size: 50Gi
      local_storage: true
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
    dataLog:
      name: datalog
      size: 10Gi
      local_storage: true
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
  extraInitContainers: {}
  ## Zookeeper configmap
  ## templates/zookeeper-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    PULSAR_MEM: >
      "
      -Xms64m -Xmx128m
      -Dcom.sun.management.jmxremote
      -Djute.maxbuffer=10485760
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+AggressiveOpts
      -XX:+DoEscapeAnalysis
      -XX:+DisableExplicitGC
      -XX:+PerfDisableSharedMem
      -Dzookeeper.forceSync=no
      "
    PULSAR_GC: >
      "
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      "
  ## Zookeeper service
  ## templates/zookeeper-service.yaml
  ##
  service:
    annotations:
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
  ## Zookeeper PodDisruptionBudget
  ## templates/zookeeper-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar: Bookkeeper cluster
## templates/bookkeeper-statefulset.yaml
##
bookkeeper:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: bookie
  ## BookKeeper Cluster Initialize
  ## templates/bookkeeper-cluster-initialize.yaml
  metadata:
    image:
      # the image used for running `bookkeeper-cluster-initialize` job
      repository: apachepulsar/pulsar-all
      tag: 2.5.2
      # repository: streamnative/platform
      # tag: v1.0.0
      pullPolicy: IfNotPresent
    ## Set the resources used for running `bin/bookkeeper shell initnewcluster`
    ##
    resources:
      # requests:
        # memory: 4Gi
        # cpu: 2
  replicaCount: 4
  ports:
    http: 8000
    bookie: 3181
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      failureThreshold: 60
      initialDelaySeconds: 10
      periodSeconds: 30
    readiness:
      enabled: true
      failureThreshold: 60
      initialDelaySeconds: 10
      periodSeconds: 30
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 60
      periodSeconds: 30
  affinity:
    anti_affinity: true
  annotations: {}
  securityContext: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 512Mi
      cpu: 0.2
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    journal:
      name: journal
      size: 10Gi
      local_storage: true
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
        # extraParameters:
        #   iopsPerGB: "50"
    ledgers:
      name: ledgers
      size: 50Gi
      local_storage: true
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-ssd
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
        # extraParameters:
        #   iopsPerGB: "50"
  extraInitContainers: {}
  ## Bookkeeper configmap
  ## templates/bookkeeper-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  placementPolicy:
    rackAware: true
  configData:
    # `BOOKIE_MEM` is used for `bookie shell`
    BOOKIE_MEM: >
      "
      -Xms128m
      -Xmx256m
      -XX:MaxDirectMemorySize=256m
      "
    # we use `bin/pulsar` for starting bookie daemons
    PULSAR_MEM: >
      "
      -Xms128m
      -Xmx256m
      -XX:MaxDirectMemorySize=256m
      "
    PULSAR_GC: >
      "
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+AggressiveOpts
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
      -XX:+PrintGCDetails
      -XX:+PrintGCTimeStamps
      -XX:+PrintGCApplicationStoppedTime
      -XX:+PrintHeapAtGC
      -verbosegc
      -Xloggc:/var/log/bookie-gc.log
      -XX:G1LogLevel=finest
      "
  ## Bookkeeper Service
  ## templates/bookkeeper-service.yaml
  ##
  service:
    annotations:
      publishNotReadyAddresses: "true"
  ## Bookkeeper PodDisruptionBudget
  ## templates/bookkeeper-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar: Bookkeeper AutoRecovery
## templates/autorecovery-statefulset.yaml
##
autorecovery:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: recovery
  replicaCount: 1
  ports:
    http: 8000
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  affinity:
    anti_affinity: true
  annotations: {}
  securityContext: {}
  # tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 128Mi
      cpu: 0.1
  extraInitContainers: {}
  ## Bookkeeper auto-recovery configmap
  ## templates/autorecovery-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    BOOKIE_MEM: >
      "
      -Xms64m -Xmx64m
      "

## Pulsar Zookeeper metadata. The metadata will be deployed as
## soon as the last zookeeper node is reachable. The deployment
## of other components that depends on zookeeper, such as the
## bookkeeper nodes, broker nodes, etc will only start to be
## deployed when the zookeeper cluster is ready and with the
## metadata deployed
pulsar_metadata:
  component: pulsar-init
  image:
    # the image used for running `pulsar-cluster-initialize` job
    repository: apachepulsar/pulsar-all
    tag: 2.5.2
    # repository: streamnative/platform
    # tag: v1.0.0
    pullPolicy: IfNotPresent
  ## set an existing configuration store
  # configurationStore:
  configurationStoreMetadataPrefix: ""

## Pulsar: KoP Protocol Handler
kop:
  ports:
    plaintext: 9092
    ssl: 9093

## Pulsar: Broker cluster
## templates/broker-statefulset.yaml
##
broker:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: broker
  replicaCount: 3
  ports:
    http: 8080
    https: 8443
    pulsar: 6650
    pulsarssl: 6651
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 60
      periodSeconds: 10
  affinity:
    anti_affinity: true
  annotations: {}
  tolerations: []
  securityContext: {}
  gracePeriod: 30
  resources:
    requests:
      memory: 512Mi
      cpu: 0.2
  extraInitContainers: {}
  ## Broker configmap
  ## templates/broker-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    PULSAR_MEM: >
      "
      -Xms128m -Xmx256m -XX:MaxDirectMemorySize=256m
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+AggressiveOpts
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
      "
    PULSAR_GC: >
      "
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      "
    AWS_ACCESS_KEY_ID: "[YOUR AWS ACCESS KEY ID]"
    AWS_SECRET_ACCESS_KEY: "[YOUR SECRET]"
    managedLedgerDefaultEnsembleSize: "3"
    managedLedgerDefaultWriteQuorum: "3"
    managedLedgerDefaultAckQuorum: "2"
  ## Broker service
  ## templates/broker-service.yaml
  ##
  service:
    annotations: {}
  ## Broker PodDisruptionBudget
  ## templates/broker-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1
  offload:
    enabled: false
    managedLedgerOffloadDriver: aws-s3
    managedLedgerMinLedgerRolloverTimeMinutes: "1"
    managedLedgerMaxEntriesPerLedger: "5000"
    gcs:
      enabled: false
      gcsManagedLedgerOffloadRegion: "[YOUR REGION OF GCS]"
      gcsManagedLedgerOffloadBucket: "[YOUR BUCKET OF GCS]"
      gcsManagedLedgerOffloadMaxBlockSizeInBytes: "67108864"
      gcsManagedLedgerOffloadReadBufferSizeInBytes: "1048576"
    s3:
      enabled: false
      s3ManagedLedgerOffloadRegion: "[YOUR REGION OF S3]"
      s3ManagedLedgerOffloadBucket: "[YOUR BUCKET OF S3]"
      s3ManagedLedgerOffloadMaxBlockSizeInBytes: "67108864"
      s3ManagedLedgerOffloadReadBufferSizeInBytes: "1048576"

## Pulsar: Functions Worker
## templates/function-worker-configmap.yaml
##
functions:
  component: functions-worker
  enableCustomizerRuntime: false
  runtimeCustomizerClassName: "org.apache.pulsar.functions.runtime.kubernetes.BasicKubernetesManifestCustomizer"
  pulsarExtraClasspath: "extraLibs"


## Pulsar: pulsar detector
## templates/pulsar-detector-statefulset.yaml
##
pulsar_detector:
  component: pulsar-detector
  replicaCount: 1

  gracePeriod: 30
  port: 9000
  ## Proxy service
  ## templates/pulsar-detector-service.yaml
  ##
  service:
    spec:
      clusterIP: None
    annotations: {}

  ## Pulsar detector PodDisruptionBudget
  ## templates/pulsar-detector-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1
## Pulsar: Proxy Cluster
## templates/proxy-statefulset.yaml
##
proxy:
  # use a component name that matches your grafana configuration
  # so the metrics are correctly rendered in grafana dashboard
  component: proxy
  replicaCount: 3
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  probe:
    liveness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 60
      periodSeconds: 10
  affinity:
    anti_affinity: true
  annotations: {}
  securityContext: {}
  tolerations: []
  gracePeriod: 30
  resources:
    requests:
      memory: 128Mi
      cpu: 0.2
  extraInitContainers: {}
  ## Proxy configmap
  ## templates/proxy-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    PULSAR_MEM: >
      "
      -Xms64m -Xmx64m -XX:MaxDirectMemorySize=64m
      -Dio.netty.leakDetectionLevel=disabled
      -Dio.netty.recycler.linkCapacity=1024
      -XX:+ParallelRefProcEnabled
      -XX:+UnlockExperimentalVMOptions
      -XX:+AggressiveOpts
      -XX:+DoEscapeAnalysis
      -XX:ParallelGCThreads=4
      -XX:ConcGCThreads=4
      -XX:G1NewSizePercent=50
      -XX:+DisableExplicitGC
      -XX:-ResizePLAB
      -XX:+ExitOnOutOfMemoryError
      -XX:+PerfDisableSharedMem
      "
    PULSAR_GC: >
      "
      -XX:+UseG1GC
      -XX:MaxGCPauseMillis=10
      "
  ## Proxy service
  ## templates/proxy-service.yaml
  ##
  ports:
    http: 8080
    https: 443
    pulsar: 6650
    pulsarssl: 6651
  service:
    annotations: {}
    type: LoadBalancer
    extraSpec: {}
  ## Proxy PodDisruptionBudget
  ## templates/proxy-pdb.yaml
  ##
  pdb:
    usePolicy: true
    maxUnavailable: 1

## Pulsar ToolSet
## templates/toolset-deployment.yaml
##
toolset:
  component: toolset
  useProxy: true
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
  ## Bastion configmap
  ## templates/bastion-configmap.yaml
  ##
  # Automtically Roll Deployments when configmap is changed
  autoRollDeployment: true
  configData:
    BOOKIE_MEM: >
      "
      -Xms64M
      -Xmx128M
      -XX:MaxDirectMemorySize=128M
      "
    PULSAR_MEM: >
      "
      -Xms64M
      -Xmx128M
      -XX:MaxDirectMemorySize=128M
      "

#############################################################
### Monitoring Stack : Prometheus / Grafana
#############################################################

configmapReload:
  prometheus:
    ## If false, the configmap-reload container will not be deployed
    ##
    enabled: true
    ## configmap-reload container name
    ##
    name: configmap-reload
    ## configmap-reload container image
    ##
    image:
      repository: jimmidyson/configmap-reload
      tag: v0.3.0
      pullPolicy: IfNotPresent

    ## Additional configmap-reload container arguments
    ##
    extraArgs: {}
    ## Additional configmap-reload volume directories
    ##
    extraVolumeDirs: []

    ## Additional configmap-reload mounts
    ##
    extraConfigmapMounts: []
      # - name: prometheus-alerts
      #   mountPath: /etc/alerts.d
      #   subPath: ""
      #   configMap: prometheus-alerts
      #   readOnly: true

    ## configmap-reload resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}
  alertmanager:
    ## If false, the configmap-reload container will not be deployed
    ##
    enabled: true

    ## configmap-reload container name
    ##
    name: configmap-reload

    ## configmap-reload container image
    ##
    image:
      repository: jimmidyson/configmap-reload
      tag: v0.3.0
      pullPolicy: IfNotPresent

    ## Additional configmap-reload container arguments
    ##
    extraArgs: {}
    ## Additional configmap-reload volume directories
    ##
    extraVolumeDirs: []

    ## Additional configmap-reload mounts
    ##
    extraConfigmapMounts: []
      # - name: prometheus-alerts
      #   mountPath: /etc/alerts.d
      #   subPath: ""
      #   configMap: prometheus-alerts
      #   readOnly: true

    ## configmap-reload resource requests and limits
    ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
    ##
    resources: {}

## Monitoring Stack: Prometheus
## templates/prometheus-deployment.yaml
##
prometheus:
  component: prometheus
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 0
  port: 9090
  resources:
    requests:
      memory: 256Mi
      cpu: 0.1
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    data:
      name: data
      size: 10Gi
      local_storage: true
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-standard
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
  args:
    ## Prometheus data retention period (default if not specified is 15 days)
    ##
    retention: "15d"
  securityContext:
    runAsUser: 65534
    runAsNonRoot: true
    runAsGroup: 65534
    fsGroup: 65534
  probe:
    liveness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10

  ## Prometheus service
  ## templates/prometheus-service.yaml
  ##
  service:
    annotations: {}

datadog:
  component: datadog
  namespace: pulsar
  components:
    zookeeper:
      enabled: false
      metrics: [
        "\"_*\""
      ]
    bookkeeper:
      enabled: false
      metrics: [
        "\"_*\""
      ]
    broker:
      enabled: false
      metrics: [
        "\"_*\""
      ]
    proxy:
      enabled: false
      metrics: [
        "\"_*\""
      ]

## Grafana dependency
##
grafana:
  component: grafana
  port: 3000
  testFramework:
    enabled: false
  ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).
  ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.
  ## ref: http://kubernetes.io/docs/user-guide/services/
  ##
  service:
    type: LoadBalancer
    port: 443
    targetPort: 3000
    # targetPort: 4181 To be used with a proxy extraContainer
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "arn:aws:acm:eu-west-1:268324876595:certificate/d65266d3-b4ab-43da-b35d-c75681ee6371"
    labels: {}
    portName: service

  resources:
    requests:
      memory: 250Mi
      cpu: 0.1

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    enabled: true
    # storageClassName: default
    size: 10Gi
    finalizers:
      - kubernetes.io/pvc-protection

  # Use an existing secret for the admin user.
  admin:
    existingSecret: "pulsar-control-center-admin-secret"
    userKey: admin-user
    passwordKey: admin-password

  ## Extra environment variables that will be pass onto deployment pods
  env:
    GF_SERVER_ROOT_URL: "https://pulsar-grafana-dev.singlespot.com"
    GF_AUTH_GITHUB_ENABLED: "true"
    GF_AUTH_GITHUB_ALLOW_SIGN_UP: "true"
    GF_AUTH_GITHUB_ALLOWED_ORGANIZATIONS: "Singlespot"

  ## "valueFrom" environment variable references that will be added to deployment pods
  ## ref: https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.17/#envvarsource-v1-core
  ## Renders in container spec as:
  ##   env:
  ##     ...
  ##     - name: <key>
  ##       valueFrom:
  ##         <value rendered as YAML>
  envValueFrom: {}

  ## The name of a secret in the same kubernetes namespace which contain values to be added to the environment
  ## This can be useful for auth tokens, etc. Value is templated.
  envFromSecret: "grafana-github-secret"

  ## Reference to external ConfigMap per provider. Use provider name as key and ConfiMap name as value.
  ## A provider dashboards must be defined either by external ConfigMaps or in values.yaml, not in both.
  ## ConfigMap data example:
  ##
  ## data:
  ##   example-dashboard.json: |
  ##     RAW_JSON
  ##
  dashboardsConfigMaps: {}
  #  default: ""

  ## Grafana's primary configuration
  ## NOTE: values in map will be converted to ini format
  ## ref: http://docs.grafana.org/installation/configuration/
  ##
  grafana.ini:
    paths:
      data: /var/lib/grafana/pulsar/data
      logs: /var/log/grafana
      plugins: /var/lib/grafana/pulsar/plugins
      provisioning: /etc/grafana/provisioning
    analytics:
      check_for_updates: true
    log:
      mode: console
    log.file:
      level: info
      # log line format, valid options are text, console and json
      format: text
    grafana_net:
      url: https://grafana.net

  ## Sidecars that collect the configmaps with specified label and stores the included files them into the respective folders
  ## Requires at least Grafana 5 to work and can't be used together with parameters dashboardProviders, datasources and dashboards
  sidecar:
    image: kiwigrid/k8s-sidecar:0.1.151
    imagePullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 100Mi
      requests:
        cpu: 100m
        memory: 100Mi
    # skipTlsVerify Set to true to skip tls verification for kube api calls
    # skipTlsVerify: true
    dashboards:
      enabled: true
      SCProvider: true
      # label that the configmaps with dashboards are marked with
      label: grafana_dashboard
      # folder in the pod that should hold the collected dashboards (unless `defaultFolderName` is set)
      folder: /tmp/dashboards
      # The default folder name, it will create a subfolder under the `folder` and put dashboards in there instead
      defaultFolderName: null
      # If specified, the sidecar will search for dashboard config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null
      # provider configuration that lets grafana manage the dashboards
      provider:
        # name of the provider, should be unique
        name: pulsar
        # orgid as configured in grafana
        orgid: 1
        # folder in which the dashboards should be imported in grafana
        folder: ''
        # type of the provider
        type: file
        # disableDelete to activate a import-only behaviour
        disableDelete: false
        editable: true
        # allow updating provisioned dashboards from the UI
        allowUiUpdates: true
        updateIntervalSeconds: 60  # how often Grafana will scan for changed dashboard
    datasources:
      enabled: true
      # label that the configmaps with datasources are marked with
      label: grafana_datasource
      # If specified, the sidecar will search for datasource config-maps inside this namespace.
      # Otherwise the namespace in which the sidecar is running will be used.
      # It's also possible to specify ALL to search in all namespaces
      searchNamespace: null


## Monitoring Stack: node_exporter
## templates/node-exporter.yaml
##
node_exporter:
  component: node-exporter
  annotations: {}
  limits:
    cpu: 10m
    memory: 50Mi
  requests:
    cpu: 10m
    memory: 50Mi

alert_manager:
  component: alert-manager
  port: 9093
  annotations: {}
  replicaCount: 1
  gracePeriod: 0
  resources:
    requests:
      memory: 250Mi
      cpu: 0.1
  service:
    spec:
      clusterIP: None
    annotations: {}
  securityContext:
    runAsUser: 65534
    runAsNonRoot: true
    runAsGroup: 65534
    fsGroup: 65534
  probe:
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 30
      periodSeconds: 10
  # alert manager config
  init:
    envFromSecret: alertmanager-secret
    image:
      repository: dibi/envsubst
      tag: 1
      pullPolicy: IfNotPresent
  config_reloader:
    name: config-reloader
    image:
      repository: shurshun/inotify-tools
      tag: latest
      pullPolicy: IfNotPresent
    resources: {}
  config:
    global:
      resolve_timeout: 1m
      slack_api_url: $SLACK_API_URL
    route:
      group_by: [alertname]
      receiver: 'slack'
      group_interval: 1m
      repeat_interval: 10m
    receivers:
      - name: slack
        slack_configs:
          - channel: '#pulsar-alerts-dev'
            icon_url: https://avatars3.githubusercontent.com/u/3380462
            send_resolved: true
            username: 'EKS DEV'
            #            title: '{{ template "custom_title" . }}'
            title: '{{ template "slack.default.title" . }}'
            pretext: '{{ .CommonAnnotations.summary }}'
            text: |-
              {{ range .Alerts }}
                *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
                *Description:* {{ .Annotations.description }}
                *Details:*
                {{ range .Labels.SortedPairs }} • *{{ .Name }}:* `{{ .Value }}`
                {{ end }}
              {{ end }}
      #            text: '{{ template "custom_slack_message" . }}'
  extraConfigFiles:
  # add alert rules below
  rules:
    groups:
      - name: Mixed
        rules:
          - alert: InstanceDown
            expr: up == 0
            for: 1m
            labels:
              severity: 'critical'
            annotations:
              title: 'Instance {{ $labels.instance }} down'
              summary: 'Instance {{ $labels.instance }} down'
              description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 1 minute.\n  LABELS: {{ $labels }}"

      - name: Nodes
        rules:
          - alert: HostOutOfDiskSpace
            expr: (node_filesystem_avail_bytes{mountpoint="/rootfs"}  * 100) / node_filesystem_size_bytes{mountpoint="/rootfs"} < 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Host out of disk space (instance {{ $labels.instance }})"
              description: "Disk is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: HostHighCpuLoad
            expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Host high CPU load (instance {{ $labels.instance }})"
              description: "CPU load is > 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: Containers
        rules:
          - alert: ContainerKilled
            expr: time() - container_last_seen > 60
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container killed (instance {{ $labels.instance }})"
              description: "A container has disappeared\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ContainerCpuUsage
            expr: (sum(rate(container_cpu_usage_seconds_total{name=~".+"}[3m])) BY (instance, name) * 100) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container CPU usage (instance {{ $labels.instance }})"
              description: "Container CPU usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ContainerMemoryUsage
            expr: (sum(container_memory_usage_bytes) BY (instance, pod, container) / sum(container_spec_memory_limit_bytes{pod=~".+",container=~".+"} > 0) BY (instance, pod, container) * 100) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container memory usage is high ({{ $labels.pod }})"
              description: "Container memory usage is above 80%\n  VALUE = {{ $value }}\n  Pod name: {{ $labels.pod }}\n  Container name: {{ $labels.container }}\n  Instance: {{ $labels.instance }}"
          - alert: ContainerVolumeUsage
            expr: (1 - (sum(container_fs_inodes_free) BY (instance) / sum(container_fs_inodes_total) BY (instance)) * 100) > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Container Volume usage (instance {{ $labels.instance }})"
              description: "Container Volume usage is above 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: ElasticSearch
        rules:
          - alert: ElasticsearchHeapUsageTooHigh
            expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 90
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch Heap Usage Too High (instance {{ $labels.instance }})"
              description: "The heap usage is over 90% for 5m\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchHeapUsageWarning
            expr: (elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"}) * 100 > 80
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch Heap Usage warning (instance {{ $labels.instance }})"
              description: "The heap usage is over 80% for 5m\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchDiskSpaceLow
            expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 20
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch disk space low (instance {{ $labels.instance }})"
              description: "The disk usage is over 80%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchDiskOutOfSpace
            expr: elasticsearch_filesystem_data_available_bytes / elasticsearch_filesystem_data_size_bytes * 100 < 10
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch disk out of space (instance {{ $labels.instance }})"
              description: "The disk usage is over 90%\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchClusterRed
            expr: elasticsearch_cluster_health_status{color="red"} == 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch Cluster Red (instance {{ $labels.instance }})"
              description: "Elastic Cluster Red status\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchClusterYellow
            expr: elasticsearch_cluster_health_status{color="yellow"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch Cluster Yellow (instance {{ $labels.instance }})"
              description: "Elastic Cluster Yellow status\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchHealthyNodes
            expr: elasticsearch_cluster_health_number_of_nodes < number_of_nodes
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch Healthy Nodes (instance {{ $labels.instance }})"
              description: "Number Healthy Nodes less then number_of_nodes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchHealthyDataNodes
            expr: elasticsearch_cluster_health_number_of_data_nodes < number_of_data_nodes
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch Healthy Data Nodes (instance {{ $labels.instance }})"
              description: "Number Healthy Data Nodes less then number_of_data_nodes\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchRelocationShards
            expr: elasticsearch_cluster_health_relocating_shards > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch relocation shards (instance {{ $labels.instance }})"
              description: "Number of relocation shards for 20 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchInitializingShards
            expr: elasticsearch_cluster_health_initializing_shards > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch initializing shards (instance {{ $labels.instance }})"
              description: "Number of initializing shards for 10 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchUnassignedShards
            expr: elasticsearch_cluster_health_unassigned_shards > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Elasticsearch unassigned shards (instance {{ $labels.instance }})"
              description: "Number of unassigned shards for 2 min\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchPendingTasks
            expr: elasticsearch_cluster_health_number_of_pending_tasks > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch pending tasks (instance {{ $labels.instance }})"
              description: "Number of pending tasks for 10 min. Cluster works slowly.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: ElasticsearchNoNewDocuments
            expr: rate(elasticsearch_indices_docs{es_data_node="true"}[10m]) < 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Elasticsearch no new documents (instance {{ $labels.instance }})"
              description: "No new documents for 10 min!\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

      - name: Kubernetes
        rules:
          - alert: KubernetesNodeReady
            expr: kube_node_status_condition{condition="Ready",status="true"} == 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes Node ready (instance {{ $labels.instance }})"
              description: "Node {{ $labels.node }} has been unready for a long time\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesMemoryPressure
            expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes memory pressure (instance {{ $labels.instance }})"
              description: "{{ $labels.node }} has MemoryPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesDiskPressure
            expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes disk pressure (instance {{ $labels.instance }})"
              description: "{{ $labels.node }} has DiskPressure condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesOutOfDisk
            expr: kube_node_status_condition{condition="OutOfDisk",status="true"} == 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes out of disk (instance {{ $labels.instance }})"
              description: "{{ $labels.node }} has OutOfDisk condition\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesJobFailed
            expr: kube_job_status_failed > 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes Job failed (instance {{ $labels.instance }})"
              description: "Job {{$labels.namespace}}/{{$labels.exported_job}} failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesCronjobSuspended
            expr: kube_cronjob_spec_suspend != 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes CronJob suspended (instance {{ $labels.instance }})"
              description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is suspended\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesPersistentvolumeclaimPending
            expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes PersistentVolumeClaim pending (instance {{ $labels.instance }})"
              description: "PersistentVolumeClaim {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is pending\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesVolumeOutOfDiskSpace
            expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes Volume out of disk space (instance {{ $labels.instance }})"
              description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesVolumeFullInTwelveHours
            expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 12 * 3600) < 0
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes Volume full in 12 hours (instance {{ $labels.instance }})"
              description: "{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesPersistentvolumeError
            expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes PersistentVolume error (instance {{ $labels.instance }})"
              description: "Persistent volume is in bad state\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesStatefulsetDown
            expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes StatefulSet down (instance {{ $labels.instance }})"
              description: "A StatefulSet went down\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesHpaScalingAbility
            expr: kube_hpa_status_condition{condition="false", status="AbleToScale"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes HPA scaling ability (instance {{ $labels.instance }})"
              description: "Pod is unable to scale\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesHpaMetricAvailability
            expr: kube_hpa_status_condition{condition="false", status="ScalingActive"} == 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes HPA metric availability (instance {{ $labels.instance }})"
              description: "HPA is not able to colelct metrics\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesHpaScaleCapability
            expr: kube_hpa_status_desired_replicas >= kube_hpa_spec_max_replicas
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes HPA scale capability (instance {{ $labels.instance }})"
              description: "The maximum number of desired Pods has been hit\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesPodNotHealthy
            expr: min_over_time(sum by (kubernetes_node, namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown|Failed"} == 1)[1h:])
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes Pod not healthy (instance {{ $labels.kubernetes_node }})"
              description: "Pod has been in a non-ready state for longer than an hour.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesPodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 5
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes pod crash looping (instance {{ $labels.instance }})"
              description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesReplicassetMismatch
            expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }})"
              description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesDeploymentReplicasMismatch
            expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }})"
              description: "Deployment Replicas mismatch\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesStatefulsetReplicasMismatch
            expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }})"
              description: "A StatefulSet has not matched the expected number of replicas for longer than 15 minutes.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesDeploymentGenerationMismatch
            expr: kube_deployment_status_observed_generation != kube_deployment_metadata_generation
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes Deployment generation mismatch (instance {{ $labels.instance }})"
              description: "A Deployment has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesStatefulsetGenerationMismatch
            expr: kube_statefulset_status_observed_generation != kube_statefulset_metadata_generation
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes StatefulSet generation mismatch (instance {{ $labels.instance }})"
              description: "A StatefulSet has failed but has not been rolled back.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesStatefulsetUpdateNotRolledOut
            expr: max without (revision) (kube_statefulset_status_current_revision unless kube_statefulset_status_update_revision) * (kube_statefulset_replicas != kube_statefulset_status_replicas_updated)
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes StatefulSet update not rolled out (instance {{ $labels.instance }})"
          - alert: KubernetesDaemonsetRolloutStuck
            expr: kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100 or kube_daemonset_status_desired_number_scheduled - kube_daemonset_status_current_number_scheduled > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes DaemonSet rollout stuck (instance {{ $labels.instance }})"
              description: "Some Pods of DaemonSet are not scheduled or not ready\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesDaemonsetMisscheduled
            expr: kube_daemonset_status_number_misscheduled > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes DaemonSet misscheduled (instance {{ $labels.instance }})"
              description: "Some DaemonSet Pods are running where they are not supposed to run\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesCronjobTooLong
            expr: time() - kube_cronjob_next_schedule_time > 3600
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes CronJob too long (instance {{ $labels.instance }})"
              description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesJobCompletion
            expr: kube_job_spec_completions - kube_job_status_succeeded > 0 or kube_job_status_failed > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes job completion (instance {{ $labels.instance }})"
              description: "Kubernetes Job failed to complete\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesApiServerErrors
            expr: sum(rate(apiserver_request_count{job="apiserver",code=~"^(?:5..)$"}[2m])) / sum(rate(apiserver_request_count{job="apiserver"}[2m])) * 100 > 3
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes API server errors (instance {{ $labels.instance }})"
              description: "Kubernetes API server is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesApiClientErrors
            expr: (sum(rate(rest_client_requests_total{code=~"(4|5).."}[2m])) by (instance, job) / sum(rate(rest_client_requests_total[2m])) by (instance, job)) * 100 > 1
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes API client errors (instance {{ $labels.instance }})"
              description: "Kubernetes API client is experiencing high error rate\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesClientCertificateExpiresNextWeek
            expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 7*24*60*60
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes client certificate expires next week (instance {{ $labels.instance }})"
              description: "A client certificate used to authenticate to the apiserver is expiring next week.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesClientCertificateExpiresSoon
            expr: apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0 and histogram_quantile(0.01, sum by (job, le) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 24*60*60
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "Kubernetes client certificate expires soon (instance {{ $labels.instance }})"
              description: "A client certificate used to authenticate to the apiserver is expiring in less than 24.0 hours.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: KubernetesApiServerLatency
            expr: histogram_quantile(0.99, sum(apiserver_request_latencies_bucket{verb!~"CONNECT|WATCHLIST|WATCH|PROXY"}) WITHOUT (instance, resource)) / 1e+06 > 1
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: "Kubernetes API server latency (instance {{ $labels.instance }})"
              description: "Kubernetes API server has a 99th percentile latency of {{ $value }} seconds for {{ $labels.verb }} {{ $labels.resource }}.\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"
          - alert: CorednsPanicCount
            expr: increase(coredns_panic_count_total[10m]) > 0
            for: 5m
            labels:
              severity: error
            annotations:
              summary: "CoreDNS Panic Count (instance {{ $labels.instance }})"
              description: "Number of CoreDNS panics encountered\n  VALUE = {{ $value }}\n  LABELS: {{ $labels }}"

## Components Stack: pulsar_manager
## templates/pulsar-manager.yaml
##
pulsar_manager:
  component: pulsar-manager
  ports:
    frontend: 9527
    backend: 7750
  replicaCount: 1
  probe:
    liveness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 10
      periodSeconds: 30
    readiness:
      enabled: true
      failureThreshold: 10
      initialDelaySeconds: 10
      periodSeconds: 30
    startup:
      enabled: false
      failureThreshold: 30
      initialDelaySeconds: 10
      periodSeconds: 30
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 0
  resources:
    requests:
      memory: 250Mi
      cpu: 0.1
  volumes:
    # use a persistent volume or emptyDir
    persistence: true
    data:
      name: data
      size: 10Gi
      local_storage: true
      ## If the storage class is left undefined when using persistence
      ## the default storage class for the cluster will be used.
      ##
      # storageClass:
        # type: pd-standard
        # fsType: xfs
        # provisioner: kubernetes.io/gce-pd
  ## Pulsar manager service
  ## templates/pulsar-manager-service.yaml
  ##
  service:
    port: 443
    spec:
      type: LoadBalancer
    annotations:
      service.beta.kubernetes.io/aws-load-balancer-ssl-cert: "arn:aws:acm:eu-west-1:268324876595:certificate/d65266d3-b4ab-43da-b35d-c75681ee6371"
  ## pulsar manager configmap
  ## templates/pulsar-manager-configmap.yaml
  ##
  configData: {}
  superuser:
    description: "Pulsar Manager Admin"
    email: support@pulsar.io
  redirect:
    scheme: https
    host: pulsar-manager-dev.singlespot.com
    port: 443
  scripts:
    backend_entrypoint:
      command: /pulsar-manager/pulsar-manager/bin/pulsar-manager
      # extra arguments
      # extraArgs:
  spring:
    datasource:
      username: pulsar
      password: pulsar
  admin_secret: pulsar-control-center-admin-secret

## Components Stack: pulsar operators rbac
## templates/pulsar-operators-rbac.yaml
##

rbac:
  enable: true
  roleName: pulsar-operator
  roleBindingName: pulsar-operator-cluster-role-binding

# Deploy pulsar sql
presto:
  coordinator:
    component: coordinator
    replicaCount: 1
    tolerations: []
    affinity:
      anti_affinity: true
    annotations: {}
    gracePeriod: 10
    ports:
      http: 8081
    resources:
      requests:
        memory: 4Gi
        cpu: 2
    # nodeSelector:
      # cloud.google.com/gke-nodepool: default-pool
    probe:
      liveness:
        enabled: true
        failureThreshold: 10
        initialDelaySeconds: 10
        periodSeconds: 30
      readiness:
        enabled: true
        failureThreshold: 10
        initialDelaySeconds: 10
        periodSeconds: 30
      startup:
        enabled: false
        failureThreshold: 30
        initialDelaySeconds: 10
        periodSeconds: 30
    config:
      http:
        port: 8081
      query:
        maxMemory: "1GB"
        maxMemoryPerNode: "128MB"
    jvm:
      memory: 2G
    log:
      presto:
        level: DEBUG
  worker:
    component: worker
    replicaCount: 2
    tolerations: []
    affinity:
      anti_affinity: true
    annotations: {}
    gracePeriod: 10
    ports:
      http: 8081
    resources:
      requests:
        memory: 4Gi
        cpu: 2
    # nodeSelector:
      # cloud.google.com/gke-nodepool: default-pool
    probe:
      liveness:
        enabled: true
        failureThreshold: 10
        initialDelaySeconds: 10
        periodSeconds: 30
      readiness:
        enabled: true
        failureThreshold: 10
        initialDelaySeconds: 10
        periodSeconds: 30
      startup:
        enabled: false
        failureThreshold: 30
        initialDelaySeconds: 10
        periodSeconds: 30
    config:
      query:
        maxMemory: "1GB"
        maxMemoryPerNode: "128MB"
    jvm:
      memory: 2G
    log:
      presto:
        level: DEBUG
  node:
    environment: production
  catalog:
    pulsar:
      maxEntryReadBatchSize: "100"
      targetNumSplits: "16"
      maxSplitMessageQueueSize: "10000"
      maxSplitEntryQueueSize: "1000"
      namespaceDelimiterRewriteEnable: "true"
      rewriteNamespaceDelimiter: "/"
      bookkeeperThrottleValue: "0"
      managedLedgerCacheSizeMB: "0"
  service:
    spec:
      type: ClusterIP

# Flag to control whether to create tenants, namespaces and topics
createTopics: true

tenants:
  - name: data-streaming
  - name: devices

namespaces:
  - name: queuing
    tenant: data-streaming
  - name: streaming
    tenant: data-streaming
  - name: filtering
    tenant: data-streaming
  - name: raw
    tenant: data-streaming
  - name: errors
    tenant: devices

partitionedTopics:
  - fullName: persistent://data-streaming/queuing/geovisits
    partitions: "{{ .Values.bookkeeper.replicaCount }}"
  - fullName: persistent://data-streaming/queuing/locations
    partitions: "{{ .Values.bookkeeper.replicaCount }}"
  - fullName: persistent://data-streaming/queuing/traces
    partitions: "{{ .Values.bookkeeper.replicaCount }}"
  - fullName: persistent://data-streaming/queuing/visits
    partitions: "{{ .Values.bookkeeper.replicaCount }}"
  - fullName: persistent://data-streaming/raw/locations
    partitions: "{{ .Values.bookkeeper.replicaCount }}"
  - fullName: persistent://data-streaming/raw/visits
    partitions: "{{ .Values.bookkeeper.replicaCount }}"

prometheusJobs:
  - jobName: copy-api
    host: pulsar-copy-metrics-dev.singlespot.com
  - jobName: geocoding-api
    host: geocoding-api-metrics-dev.singlespot.com
  - jobName: producer-api
    host: pulsar-producer-metrics-dev.singlespot.com

servicesNamespace: pulsar-services-dev
